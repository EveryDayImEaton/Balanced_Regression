{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey Couse and Abraham Eaton <br>\n",
    "22-Jan-2021<br>\n",
    "\n",
    "### Overview:\n",
    "Machine learning hyperparameters are optimized as a part of the training process to increase the performance of models.  An area less researched however has been how to optimize the selection of the training and validation datasets.  Often this is done randomly, in order to \"fairly\" address the data.  Standard practice involves balancing a dataset across the dependent (predicted) variable when a classification problem has unbalanced classes.  This is done through oversampling, undersampling, sample weighting, or other methods.  In practice this helps correct for unbalanced data and yields better results.  \n",
    "<br>\n",
    "We believe that this mentality should be taken one step further by balancing training data across a number of different features not limited to only the dependent variable.  Our hypothesis is that ensuring that the training set and validation set are loosely balanced will allow machine learning models to more accurately distill the true signals in the training data.  Particularly with small datasets, small samples sizes can lead to training or validation sets that are different from each other in potentially critical ways.  In this script we attempt to test this hypothesis using datasets from the machine learning respository.  \n",
    "<br>\n",
    "\n",
    "### Sources:\n",
    "This script utilizes a UCI Machine Learning Repository API developed and maintained here: https://github.com/tirthajyoti/UCI-ML-API#lowlevelfunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics, DataFrames, LinearAlgebra, CSV, Random, ScikitLearn, MatrixImpute\n",
    "using ScikitLearn.Pipelines: Pipeline, make_pipeline\n",
    "using Lathe.preprocess: TrainTestSplit, OneHotEncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sk_import preprocessing: normalize\n",
    "@sk_import preprocessing: StandardScaler\n",
    "@sk_import preprocessing: MinMaxScaler\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import linear_model: LogisticRegression\n",
    "@sk_import linear_model: LinearRegression\n",
    "@sk_import metrics: (mean_squared_error, r2_score)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = @__DIR__;\n",
    "datapath = string(path[1:end-4],\"Data\");\n",
    "outputpath = string(path[1:end-4],\"Output\");\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if length(readdir(datapath)) == 0;\n",
    "    error(\"No data files in data directory, run load_UCI_data.py first!\")\n",
    "else\n",
    "    Folder_Array = readdir(datapath);\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct DataSet\n",
    "    name::String\n",
    "    data::DataFrame\n",
    "    \n",
    "    ## Splits of the ds.data attribute\n",
    "    X_train::Array\n",
    "    y_train::Array\n",
    "    X_train_scaled::Array\n",
    "    y_train_scaled::Array\n",
    "    \n",
    "    X_valid::Array\n",
    "    y_valid::Array\n",
    "    X_valid_scaled::Array\n",
    "    y_valid_scaled::Array\n",
    "    \n",
    "    X_test::Array\n",
    "    y_test::Array\n",
    "    X_test_scaled::Array\n",
    "    y_test_scaled::Array\n",
    "    \n",
    "    \n",
    "    ## How much to put in each set of data\n",
    "    test_perc::Float64\n",
    "    valid_perc::Float64\n",
    "    \n",
    "    ## Keep track of how the data was scaled\n",
    "    scale_type::String\n",
    "    \n",
    "    ## empty after cleaning?\n",
    "    empty_after_cleaning::Bool\n",
    "    \n",
    "    DataSet(name::String) = new(name, DataFrame(), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        Array{Float64}(undef,0,0), Array{Float64}(undef,0,0), \n",
    "        0.15, 0.15, \"\",false)\n",
    "end\n",
    "\n",
    "## Establish an array of datasets\n",
    "datasets = Array{DataSet}(undef,length(Folder_Array))\n",
    "\n",
    "## Fill each element of the array with a \"DataSet\" construct, supplying the name for each\n",
    "for i in range(1,stop=length(Folder_Array))\n",
    "    datasets[i] = DataSet(Folder_Array[i])\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions that operate on a \"DataSet\" construct\n",
    "\n",
    "function load_dataset(ds::DataSet)\n",
    "    file = readdir(string(datapath,\"//\",ds.name))[1];\n",
    "    ds.data = CSV.read(string(datapath,\"//\",ds.name,\"//\",file),missingstrings=[\"-999\", \"NA\",\"NaN\",\"?\"]);\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function clean(ds::DataSet)\n",
    "    \n",
    "    \n",
    "    ## drop rows with \"missing\" as a cell value\n",
    "    #println(MatrixImpute.Impute(convert(Matrix,ds.data),4))\n",
    "    ds.data = dropmissing(ds.data)\n",
    "    \n",
    "    \n",
    "    ## room for improvement--build out something that detects categorical vairables and \n",
    "    ## ideally takes the top N categories and makes them one-hot, or just converts the \n",
    "    ## column if there are less than N categories.\n",
    "    \n",
    "#     scaled_feature = OneHotEncode(ds,:Status)\n",
    "#     select!(df, Not([:Status,:Country]))\n",
    "    \n",
    "    \n",
    "    ## only allow float or integer values (an oversimplification, see above for Categorical discussion)\n",
    "    ds.data = ds.data[:,((eltype.(eachcol(ds.data)) .== Int64) .| (eltype.(eachcol(ds.data)) .== Float64))]\n",
    "    #ds.data = complete_cases(ds.data)\n",
    "    if (size(ds.data)[1]<=10) | (size(ds.data)[2]<=2)\n",
    "        ds.empty_after_cleaning = true\n",
    "        println(\"Not enough data after cleaning to regress.\")\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "function unload_dataset(ds::DataSet)\n",
    "    ## To reduce memory for larger sets and reset the DataSet construct\n",
    "    ds.data = DataFrame()\n",
    "    \n",
    "    \n",
    "    ds.X_train= Array{Float64}(undef,0,0)\n",
    "    ds.y_train= Array{Float64}(undef,0,0)\n",
    "    ds.X_train_scaled= Array{Float64}(undef,0,0)\n",
    "    ds.y_train_scaled= Array{Float64}(undef,0,0)\n",
    "    \n",
    "    ds.X_valid= Array{Float64}(undef,0,0)\n",
    "    ds.y_valid= Array{Float64}(undef,0,0)\n",
    "    ds.X_valid_scaled= Array{Float64}(undef,0,0)\n",
    "    ds.y_valid_scaled= Array{Float64}(undef,0,0)\n",
    "    \n",
    "    ds.X_test= Array{Float64}(undef,0,0)\n",
    "    ds.y_test= Array{Float64}(undef,0,0)\n",
    "    ds.X_test_scaled= Array{Float64}(undef,0,0)\n",
    "    ds.y_test_scaled= Array{Float64}(undef,0,0)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(datasets[1]);\n",
    "clean(datasets[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function split(ds::DataSet)\n",
    "    if ds.empty_after_cleaning == false\n",
    "        \n",
    "        train, test = TrainTestSplit(ds.data,(1-ds.test_perc))\n",
    "        valid, train = TrainTestSplit(train,((ds.valid_perc)/(1-ds.test_perc)))\n",
    "\n",
    "\n",
    "        ds.X_train = train[:,1:(size(ds.data)[2]-1)]\n",
    "        ds.y_train = train[:,size(ds.data)[2]]\n",
    "\n",
    "        ds.X_valid = valid[:,1:(size(ds.data)[2]-1)]\n",
    "        ds.y_valid = valid[:,size(ds.data)[2]]\n",
    "\n",
    "        ds.X_test = test[:,1:(size(ds.data)[2]-1)]\n",
    "        ds.y_test = test[:,size(ds.data)[2]]\n",
    "\n",
    "        ds.y_train = reshape(ds.y_train,(size(ds.y_train)[1],1))\n",
    "        ds.y_valid = reshape(ds.y_valid,(size(ds.y_valid)[1],1))\n",
    "        ds.y_test = reshape(ds.y_test,(size(ds.y_test)[1],1))\n",
    "    \n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `convert(::Type{Array}, df::AbstractDataFrame)` is deprecated, use `convert(Matrix, df)` instead.\n",
      "│   caller = setproperty! at Base.jl:21 [inlined]\n",
      "└ @ Core .\\Base.jl:21\n",
      "┌ Warning: `convert(::Type{Array}, df::AbstractDataFrame)` is deprecated, use `convert(Matrix, df)` instead.\n",
      "│   caller = setproperty! at Base.jl:21 [inlined]\n",
      "└ @ Core .\\Base.jl:21\n",
      "┌ Warning: `convert(::Type{Array}, df::AbstractDataFrame)` is deprecated, use `convert(Matrix, df)` instead.\n",
      "│   caller = setproperty! at Base.jl:21 [inlined]\n",
      "└ @ Core .\\Base.jl:21\n"
     ]
    }
   ],
   "source": [
    "split(datasets[1]);\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function scale(ds::DataSet)\n",
    "    if ds.empty_after_cleaning == false\n",
    "        if ds.scale_type == \"MinMax\"\n",
    "            scaler = MinMaxScaler();\n",
    "            yscaler = MinMaxScaler();\n",
    "        elseif ds.scale_type == \"Normalize\"  ## not working currently\n",
    "            scaler = normalize();\n",
    "            yscaler = normalize();\n",
    "        elseif ds.scale_type == \"Standard\"\n",
    "            scaler = StandardScaler();\n",
    "            yscaler = StandardScaler();\n",
    "        elseif ds.scale_type == \"\"\n",
    "            println(\"default Standard Scaler being used\")\n",
    "            scaler = StandardScaler();\n",
    "            yscaler = StandardScaler();\n",
    "        else\n",
    "            println(\"Scaler type not recognized, try 'Normalize' or 'Standard' or 'MinMax'\")\n",
    "            return\n",
    "        end\n",
    "\n",
    "        fit!(scaler,ds.X_train);\n",
    "\n",
    "        ds.X_train_scaled = transform(scaler,ds.X_train);\n",
    "        ds.X_valid_scaled = transform(scaler,ds.X_valid);\n",
    "        ds.X_test_scaled = transform(scaler,ds.X_test);\n",
    "\n",
    "        fit!(yscaler,ds.y_train);\n",
    "\n",
    "        ds.y_train_scaled = transform(yscaler,ds.y_train);\n",
    "        ds.y_valid_scaled = transform(yscaler,ds.y_valid);\n",
    "        ds.y_test_scaled = transform(yscaler,ds.y_test);\n",
    "        return\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[1].scale_type = \"MinMax\"\n",
    "scale(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fit_model(ds::DataSet)\n",
    "    if (ds.empty_after_cleaning == false)\n",
    "        ## can add functionality later for different types of models\n",
    "        if true\n",
    "            model = LinearRegression()\n",
    "        end\n",
    "\n",
    "        fit_model = fit!(model,ds.X_train,ds.y_train)\n",
    "\n",
    "        predictions = fit_model.predict(ds.X_test)\n",
    "\n",
    "        # The coefficients\n",
    "        println(\"Coefficients: \\n\", fit_model.coef_)\n",
    "        println()\n",
    "        # The mean squared error\n",
    "        println(\"Mean squared error: \\n\", mean_squared_error(ds.y_test, predictions))\n",
    "        println()\n",
    "        # The coefficient of determination: 1 is perfect prediction\n",
    "        println(\"Coefficient of determination: \\n\", r2_score(ds.y_test, predictions))\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [(\"normalize\",normalize()),(\"reduce_dim\", PCA()), (\"logistic_regression\", LogisticRegression())]\n",
    "# clf = Pipeline(estimators)\n",
    "# fit!(clf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "[-0.008972949320528011 0.23669992924633915 0.014515397820578044 0.004114840970287457 -0.03634268197145784 0.6035499702524569 0.058659845085849135 -8.36477626479504e-5 -0.24537776461370517 0.5092901582604304 -0.24289301826605048 0.32483644154124774 -0.030131100197043854 -0.050867552415045376 0.04519181485674417 -0.07839186882494921 -0.4676395564813209 -0.09293138757092609 -0.006551609856056274 0.19317587930904148 1.3625896506528286e-5 0.20948014106124435]\n",
      "\n",
      "Mean squared error: \n",
      "1.0040214683866393\n",
      "\n",
      "Coefficient of determination: \n",
      "0.8463856853017001\n"
     ]
    }
   ],
   "source": [
    "fit_model(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unload_dataset(datasets[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "[-0.005325740734393695 0.224143045262405 0.01328947038410776 0.003912074435558345 -0.04008351617194593 0.6195236502073466 0.053768131816831606 -0.00027929292292136606 -0.3019560185788868 0.6290825905687777 -0.17840636499592688 0.34404131407297867 -0.026366594660104097 -0.06297801327417454 0.03815638867861296 -0.07358749355415155 -0.27012069062354943 -0.13682586286899376 -0.006402106614226001 0.18501872737431072 -2.862249890652369e-5 0.21086954381748488]\n",
      "\n",
      "Mean squared error: \n",
      "0.8868756360175812\n",
      "\n",
      "Coefficient of determination: \n",
      "0.8551188582550677\n"
     ]
    }
   ],
   "source": [
    "load_dataset(datasets[1]);\n",
    "clean(datasets[1]);\n",
    "split(datasets[1]);\n",
    "datasets[1].scale_type = \"Standard\";\n",
    "scale(datasets[1]);\n",
    "fit_model(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "Large_csv_Opinion_Corpus_for_Lebanese_Arabic_Reviews_%28OCLAR%29\n",
      "\n",
      "Not enough data after cleaning to regress.\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_QSAR_biodegradation\n",
      "\n",
      "Coefficients: \n",
      "[-0.4765238310082644 0.31144397557153575 0.6146997454201498 -0.11079294190266932 0.034865149081312816 0.944861569128173 0.05314257239785255 0.04985551974376269 -0.17721689668397878 -0.05007588246855228 0.05814983809606156 0.017947915857807556 1.5915611515058814 -0.07237058456791842 -1.0096541153757639 -0.04572221055145353 3.305802382822178 38.69258858835163 -0.3103084291826603 -0.42731137312005507 -0.07575270028894521 0.9313573564356805 -0.0662004356287588 -0.24019749795649936 0.32970865363696017 -0.28677407985002484 2.9059889421178293 0.2054157181464794 1.1053941965574032 0.0071927775743400296 -0.04013595166039375 0.009943863624683805 0.13276074576534225 0.008615433841276544 -0.0281571413911846 -0.19239688494478818 -0.030054005626116997 -0.4322163086577883 -0.10156387734403682 0.17913096902174108]\n",
      "\n",
      "Mean squared error: \n",
      "0.9262591330959647\n",
      "\n",
      "Coefficient of determination: \n",
      "0.8742396815330775\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_Seoul_Bike_Sharing_Demand\n",
      "\n",
      "Coefficients: \n",
      "[1.92024326029905e-5 0.0027897828363541697 0.010365640184778115 0.011662482956813694 -0.0017008029219273343 -7.717649054376521e-6 -0.024579515021741057 0.06507451680283156 -0.0091739206764109]\n",
      "\n",
      "Mean squared error: \n",
      "0.14177387857175258\n",
      "\n",
      "Coefficient of determination: \n",
      "0.07898563959959182\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_Shill_Bidding_Dataset\n",
      "\n",
      "Coefficients: \n",
      "[-7.356975298281985e-7 -1.7438149597425314e-6 -0.0032063721971023014 0.004489559116596757 0.9547729083803111 0.04008124166450389 0.0029469258893662065 0.006247023869309592 -0.029177344377382536 0.047193922189352935 0.0017559460605978178]\n",
      "\n",
      "Mean squared error: \n",
      "0.016149872136105366\n",
      "\n",
      "Coefficient of determination: \n",
      "0.8325062503459981\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_SkillCraft1_Master_Table_Dataset\n",
      "\n",
      "Coefficients: \n",
      "[1.5404220708148032e-9 3.68359012085136e-6 -1.7151093105779048e-7 3.388989387739872e-7 -9.624925389516017e-11 -1.956595136041668e-6 0.011129301056016911 0.09463506769708717 -2.209824356609421e-6 -0.014859084088413738 0.01892020738552575 0.0387562403574756 2.0385683045226505e-7 -8.946689785857931e-7 2.8387957822360818e-5 1.562352807609668e-6 -0.027365914596973677 3.950741279115582e-6 1.430220587608212]\n",
      "\n",
      "Mean squared error: \n",
      "4.645526892437198e-8\n",
      "\n",
      "Coefficient of determination: \n",
      "0.41607023039365276\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_Tarvel_Review_Ratings\n",
      "\n",
      "Coefficients: \n",
      "[0.3005485374231097 -0.02526222507147207 0.005096182415082783 0.04327098347655656 0.08666260299432794 -0.09451529954646495 0.04247492671819757 0.08724704827792008 -0.08804365959798613 -0.04528736876978211 0.02530787467329684 -0.12462235787034202 -0.014993872188848524 -0.062164599699575926 0.06881616648934734 0.06344727834978785 -0.010599350640191657 0.01030549980553687 0.27259456874470744 0.20805890585018424]\n",
      "\n",
      "Mean squared error: \n",
      "1.1518275359246566\n",
      "\n",
      "Coefficient of determination: \n",
      "0.3135058384378768\n",
      "\n",
      "\n",
      "Name:\n",
      "Large_csv_Turkiye_Student_Evaluation\n",
      "\n",
      "Coefficients: \n",
      "[0.0074127786870252736 -0.004446366126831323 0.002990108295883644 0.007209022675195044 -0.0059051971255885865 -0.013034750721298827 0.007730346332707821 0.021391668543328836 0.02189257635517632 -0.015532553740447274 0.06833915825050157 -0.02550173910354999 -0.06962203648645225 0.009059024408992108 -0.031582048193912346 0.004714360874190416 0.01641734757772247 0.08933607259716415 -0.016522654826834435 -0.0018761272782008703 0.03614782942085904 0.13052592816030392 -0.05045384463090003 -0.012638536355769184 0.13375539304814307 0.05236109718356208 0.17939589749004667 -0.042114030249254066 -0.015697918884277372 0.19983947079866538 0.17085471844730205 0.1462902048397819]\n",
      "\n",
      "Mean squared error: \n",
      "0.21473144409702558\n",
      "\n",
      "Coefficient of determination: \n",
      "0.8707433398011095\n",
      "\n",
      "\n",
      "Name:\n",
      "Small_csv_Cervical_Cancer_Behavior_Risk\n",
      "\n",
      "Coefficients: \n",
      "[-0.043929788751550805 0.038369407675265345 0.013744772998754467 -0.011411447682475494 -0.025173455048509125 0.06036544183277818 -0.02565997559094909 -0.07935052552386983 0.001529359440118406 0.03145533403634395 -0.05530659898631577 -0.009415455288829838 -0.01754972580851337 0.025626928491835368 -0.10299462641485412 0.06080138575997508 -0.03400569664739389 -0.008479898681102173 -0.013217587553541297]\n",
      "\n",
      "Mean squared error: \n",
      "0.1113819475982881\n",
      "\n",
      "Coefficient of determination: \n",
      "0.4696097733414851\n",
      "\n",
      "\n",
      "Name:\n",
      "Small_csv_Container_Crane_Controller_Data_Set\n",
      "\n",
      "Not enough data after cleaning to regress.\n",
      "\n",
      "\n",
      "Name:\n",
      "Small_csv_COVID-19_Surveillance\n",
      "\n",
      "Not enough data after cleaning to regress.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## loop through each DataSet and apply the load_dataset, clean, split, scale, fit_model, unload pipeline in sequence.\n",
    "\n",
    "for i in range(5,stop=14)\n",
    "    println(\"Name:\")\n",
    "    println(datasets[i].name)\n",
    "    println()\n",
    "    load_dataset(datasets[i]);\n",
    "    clean(datasets[i]);\n",
    "    split(datasets[i]);\n",
    "    datasets[i].scale_type = \"Standard\";\n",
    "    scale(datasets[i]);\n",
    "    fit_model(datasets[i])\n",
    "    unload_dataset(datasets[i]);\n",
    "    println()\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
